计算机科学丛书

# 数据结构与算法——Python语言实现

> Michael T. Goodrich
>
> Roberto Tamassia	write

#### Data Structures and Algorithms in Python

## Contents

> 第一章 Python入门
>
> 第二章 面向对象编程
>
> 第三章 算法分析
>
> 第四章 递归
>
> 第五章 基于数组的序列
>
> 第六章 栈、队列和双端队列
>
> 第七章 链表
>
> 第八章 树
>
> 第九章 优先级队列
>
> 第十章 映射、哈希表和跳跃表
>
> 第十一章 搜索树
>
> 第十二章 排序与选择
>
> 第十三章 文本处理
>
> 第十四章 图算法
>
> 第十五章 内存管理与B树



# 正文

第一章与第二章已经写入Python note中。

## 算法分析

> 一个好的算法，其时间复杂度与空间复杂度都是合理的。

### 时间复杂度

通常我们用以下七种函数来描述一个算法随输入规模时间变化趋势，具体函数如下，其增长速率依次增大，因此，设计一个好的算法，其时间复杂度应该尽可能是前四个函数，通常在n很小时，时间复杂度为二次函数的算法也可以是优秀的算法。

> 1. 常数函数 $ f(n) = c $​​ 
> 2. 对数函数 $ f(n) = \log n $​ 
> 3. 线性函数 $ f(n) = n $​​ 
> 4. $n\log n$函数  $ f(n) = n \log n $​​ 
> 5. 二次函数$ f(n) = n^2 $​ 
> 6. 三次函数及多项式 $ f(n) = n^3 $​ 
> 7. 指数函数 $ f(n) = b^n $​ 

**任何一个运行时间满足 $ O(n \log n)$​ 的算法（在计算机科学中，$\log$​​​指以2的底的对数）都是高效的。** 

### 空间复杂度

> 空间复杂度是对一个算法在运行过程中临时占用的存储空间大小的量度。一个算法在计算机存储器上所占用的存储空间包括存储**算法本身**所占用的存储空间、算法的**输入输出**数据所占用的存储空间和算法在运行过程中**临时占用**的存储空间三个方面。算法的输入输出数据所占用的存储空间是由要解决的问题决定的，是通过参数表由调用函数传递而来的，他它不随算法不同而改变。

存储算法本身所占用的存储空间与算法书写的长度成正比，要压缩这方面的存储空间，就必须编写出短的算法。算法在运行过程中临时占用的存储空间因算法而异，有的算法只需要占用少量的临时工作单元，而且不随问题规模的大小而改变，称这种算法是“就地”进行的，是节省存储的算法。

当一个算法的空间复杂度为一个常量，即不随被处理数据量 n 的大小而改变时，可表示为 $O(1)$；其他情况的表示，也与时间复杂度的表示方法一致。

### 渐近分析

> 在算法分析中，我们通常将n定义为输入数据规模，采用宏观方法把运行时间是为输入规模n的函数，同时忽略其低阶因子。而对于常用来描述时间复杂度的函数增长率比较，渐近趋势分析，通常有大 $ O $ 、大Θ和大Ω三种说法。

#### 大 $ O $ 说法

> 令 $f(n)$ 和 $g(n)$ 作为正实数映射正整数的函数，如果有实型常量 $ c > 0 $ 和整型常量 $ n_0 \geq 1$ 满足：
> $$
> f(n) \le n g(n), \quad当 n \ge n_0
> $$
> 我们就说 $f(n)$ 是 $ O(g(n))$，这种定义就是常说的大 $ O $ 说法，其意为一个函数的增长率小于或等于另一个函数（上界）。

#### 大 $ \Omega $ 说法

> 令 $f(n)$ 和 $g(n)$ 作为正实数映射正整数的函数，如果 $f(n) 是 O(g(n)) $ ，即存在实常数 $ c > 0 $ 和整型常量 $ n_0 \geq 1$ 满足：
> $$
> f(n) \ge c g(n), \quad当 n \ge n_0
> $$
> 我们就说 $f(n)$ 是 $ \Omega(g(n))$，这种定义就是常说的大 $ \Omega $ 说法其意为一个函数的增长率大于或等于另一个函数（下界）。

####  大 $ \Theta $ 说法

> 当给定一个常数因子时，两个函数的增长效率相同，如果 $f(n) 是 O(g(n)) $ ，且  $f(n) 是 \Omega (g(n)) $ ，即存在实常数 $ c' > 0 $ 、$ c'' > 0 $ 和一个整型常量 $ n_0 \geq 1$ 满足：
> $$
> c' g(n) \le f(n) \le c''g(n), \quad当 n \ge n_0
> $$
> 我们就说 $f(n)$ 是 $ \Theta(g(n))$，这种定义就是常说的大 $ \Theta $ 说法。



## 递归

> 在计算机程序中，描述迭代的一种方法是使用循环，另一种方法则是递归。
>
> **递归是一种技术，这种技术通过一个函数在执行一次或者多次调用其本身，或者一种数据结构在其表示中依赖于相同类型的结构更小的实例。**在计算中，递归提供了用于执行迭代任务的优雅并且强大的替代方案。当函数的一次调用需要进行递归调用时，该调用就会被挂起，直至递归调用完成。
>
> 下面通过四个例子学习递归：阶乘函数、英式标尺、二分查找、文件递归。

### 阶乘函数

对于一个简单的递归函数，数学表示为 $ n !$ ，它被定义为1到n的乘积，更正式的定义为：
$$
n! = \begin{cases} 1, \qquad \qquad \qquad \qquad \qquad \quad n = 0  \\ 
								n \times(n-1)\times\dots3\times2\times1 \quad n \geq 1\end{cases}
$$
递归的python实现：

```python
def factorial(n):
  if n == 0:
    return 1
  else:
    return n * factorial(n-1)
```

我们用**递归追踪**来说明一个递归函数的执行过程。在Python中，每一个函数被调用，都会创建一个被称为**活动记录**或**框架**的结构来存储信息，这个活动记录包含了一个用来存储函数调用的参数和局部变量的命名空间，以及关于在这个函数体中当前正在执行的命令的信息。

---

### 英式标尺

英式标尺是分形的一个简单示例，下面直接给出实现代码:

```python
def draw_line(tick_length, tick_label=''):
  """Draw one line with given tick length (followed by optional label)."""
  line = "-" * tick_length
  if tick_label:
    line += ''+tick_label
  print(line)
  
def draw_interval(center_length):
  """Draw tick interval based upon a center tick length."""
  if center_length > 0:
    draw_interval(center_length - 1)
    draw_line(center_length)
    draw_interval(center_length - 1)
    
def draw_ruler(num_inches, major_length):
  """Draw English ruler with given number of inches, major tick length."""
  draw_line(major+length, "0"):
    for i in range(1, 1 + num.inches):
      draw_interval(major_length - 1)
      draw_line(major_length, str(i))
```

---

### 二分查找

> 二分查找用于在一个含有n个元素的有序序列中有效地定位目标值。这是最重要的计算机算法之一，也是我们经常顺序存储数据的原因。

当序列无序时，寻找一个目标值的标准方法是使用循环来检查每一个元素，直至找到目标值或者检查完数据集的每个元素，这种方法称为**顺序查找算法**。因为最坏的情况下每个元素都要检查，这个算法的时间复杂度是 $ O(n)$ （即线性的时间）。

当**序列有序**并且可以通过索引访问时，有一个更加有效的算法。对于任意索引j，我们知道在索引0到j-1上存储的所有值都小于j，并且其后所有值都大于索引j上的值。在搜索目标值时，这使我们能快速定位到目标值。在查找时，如果不能排查一个元素与目标值相匹配，则称这个元素为候选项。该算法维持两个参数low和high，这样可使所有的候选条目的索引值都在这之间。首先，$low=0$​和$high=n-1$ 。我们可以比较目标值和中间值候选项，即索引项[mid]的数据。

随后根据三种不同情况，对应执行直至求得结果或无解，这种算法被称为二分查找法，二分查找法的时间复杂度为$O(\log n)$​以下给出代码实现。

```python
def binary_search(data, target, low, high):
    """Return True if target is found in indicate portion of a Python list.
    
    The search only considers the portion from data[low] to data [high] inclusive.
    
    """
    if low > high:
        return False
    else:
        mid = (low + high) // 2
        if target == data[mid]:
            return True
        elif target < data[mid]:
            # recur on the portion left of the middle
            return binary_search(data, target, low, mid - 1 )
        else:
            # recur on the portion right of the middle
            return binary_search(data, target, mid + 1, high)
```

---

### 文件递归

现代操作系统用递归的方式来定义文件系统目录。下面是一个抱告文件系统磁盘使用情况的递归函数。

```python
import os 

def disk_usage(path):
    """Return the number of bytes used by a file/folder and any descendents."""
    total = os.path.getsize(path)
    if os.path.isdir(path):
        for filename in  os.listdir(path):
        childpath = os.path.join(path, filename)
        total += disk_usage(childpath)

    print("{:<7}".format(total), path)
    return total
```

---

### 分析递归算法

关于上述四种递归算法的时间复杂度分析，此处不予分析，不再赘述。

### 递归算法的特点

> 递归算法可以使我们能够简洁地利用重复结构呈现诸多问题，可以避开复杂的案例分析和嵌套循环。这种方法会得出可读性更强的算法描述，而且十分有效。
>
> 虽然递归是一种非常强大的工具，但它也非常容易被误用。在递归不佳的情况下，递归求解的效率极其低下，其时间复杂度趋近于$O(2^n)$ 。另一种误用，则是无限递归，递归没有最终的一种基本情况，此时无限递归会迅速消耗计算资源（这个问题可以通过生成生成器解决。）。为了避免这种情况，Python设计者设置了递归上限，当递归超过1000次时，Python解释器将会返回RuntimeError消息，超过Python最大递归深度，当然这个深度可以通过sys库的setrecursionlimit来实现。

### 递归分类

> 如果一个递归最多开始调用一个其他递归，这种递归称为**线性递归**；
>
> 如果一个递归最多开始调用两个其他递归，这种递归称为**二路递归**；
>
> 如果一个递归最多开始调用三个或者更多其他递归，这种递归称为**多重递归**；

### 设计递归算法

一般来说，使用递归的算法通常具有以下形式：

> 1. 对于基本情况的测试。首先测试一组基本情况（至少应该有一个）。这些基本情况应该被定义，以便每个可能的递归调用链最终会达到一种基本情况，并且每个基本情况的处理不应使用递归。
> 2. 递归。如果是一种基本情况，则执行一个或多个递归调用。这个递归步骤可能包括一个测试，该测试决定执行哪几种可能的递归调用。我们应该定义每个可能的递归调用，以便使调用向一种基本情况靠近。

下面给出排列组合的代码实例：

```python
def PuzzleSolve(U, k=None, S=None, results=None, initialize=False):
	"""Solve puzzle automatically, return list of results."""
	if not initialize:
		import math
		k = len(U)
		S = []
		results = []
		initialize = True
		print(f"{math.factorial(k)} solutions in total.")
		if type(U) == str:
			U = list(U)
	backup = U.copy()
	for i in range(k):
		S.append(U.pop(i))
		if len(U) == 0:
			results.append("".join(S))
		else:
			PuzzleSolve(U, k-1, S, results, initialize)
		S.pop()
		U = backup.copy()
	return results
```

### 尾递归

> 如果执行的任何递归调用是在这种情况下的最后操作，而且通过封闭递归，递归调用的返回值立即返回，那么这就是一个尾递归。尾递归一定是线性递归。**尾递归是把变化的参数传递给递归函数的变量了**，形式上只需要在最后返回调用自身即可（不可进行任何运算）。

#### 原理

>  当编译器检测到一个函数调用是尾递归的时候，它就覆盖当前的活动记录而不是在栈中去创建一个新的。编译器可以做到这点，因为递归调用是当前活跃期内最后一条待执行的语句，于是当这个调用返回时栈帧中并没有其他事情可做，因此也就没有保存栈帧的必要了。通过覆盖当前的栈帧而不是在其之上重新添加一个，这样所使用的栈空间就大大缩减了，这使得实际的运行效率会变得更高。



## 数组

> 计算机主存由位信息组成，这些位组成更大的信息，这些单元则取决于更加精准的系统架构，典型的，一个单元就是一个字节，由8位组成。在系统中，一组相关变量能够一个接一个第存储在计算机存储器的一块连续区域内。**这样的表示方法称为数组。（数组不是一种数据结构）**

### 动态数组

> 在计算机系统中，当创建低层次数组时，必须明确声明数组的大小，以便系统分配连续的内存。由于系统会占用相邻的内存用于存储其他数据，因此其数组大小不能无限增加，对于不可变类型，如元组与字符串，当其实例化时，其数组大小已经固定，但对于可变类型，如列表类型，则依赖于一种称为动态数组的算法技巧。
>
> 一张列表通常关联着一个底层数组，该数组通常比列表的长度更长（由此可知，数组实际大小与列表大小不一致），当预留的单元（往往是4个单元）被耗尽时，系统会回收初始化原有列表并新建一个更大的列表（同时，当删除元素时，Python会**不定时**地收缩底层数组大小）。
>
> 动态数组通过分配一个更大的新数组（2倍长度），复制原有的引用，设置别名，最后再新添元素来实现。

**示例：**

```python
import sys
data = []
for k in range(10):	# n to be times you want to operate
  a = len(data)
  b = sys.getsizeof(data)	# size of list doesn't include real size of element but id
  print("Length: {:3d}; Size: {:4d}".format(a, b))
  data.append(None)
```

**Output:**

```python
Length:   0; Size:   56
Length:   1; Size:   88
Length:   2; Size:   88
Length:   3; Size:   88
Length:   4; Size:   88
Length:   5; Size:  120
Length:   6; Size:  120
Length:   7; Size:  120
Length:   8; Size:  120
Length:   9; Size:  184
```

### 摊销分析

> 动态数组的扩展，看起来效率是很低的一件事情，但实际上，其效率是很高的，这里我们采用一种叫做摊销的算法设计模式进行证明，其大体操作如下图所示。

![image-20210821124856229](E:/工具/Typora/Temp/image-20210821124856229.png)

数组按几何增长和按固定数量增长的对比

<table border="1" cellpadding="1" cellspacing="1" style="width:459px;"><tbody><tr><td style="width:144px;">几何增长(扩大2倍）</td><td style="width:58px;">b</td><td style="width:53px;">2b</td><td style="width:55px;">4b</td><td style="width:56px;">8b</td><td style="width:57px;">......</td><td style="width:54px;">mb(m=n/b)</td></tr><tr><td style="width:144px;">固定数量增长(固定增量为c）</td><td style="width:58px;">c</td><td style="width:53px;">2c</td><td style="width:55px;">3c</td><td style="width:56px;">4c</td><td style="width:57px;">......</td><td style="width:54px;">mc(m=n/c)</td></tr></tbody></table>

具体理论不予罗列，可自行分析验证。

### 序列类型效率分析

#### 常规时间操作

> 列表和元组类nonmutating行为的渐近性能。下表中 n 表示对应序列的长度， k 表示被搜索值在最左边出现时的索引

| 操作              | 时间复杂度       |
| ----------------- | ---------------- |
| len(data)         | $ O(1) $         |
| data[j]           | $ O(1) $         |
| data.count(value) | $ O(n) $         |
| data.index(value) | $ O(k + 1) $     |
| value in data     | $ O(k + 1) $     |
| data1 == data2    | $ O(k + 1) $     |
| data[j:k]         | $ O(k - j + 1) $ |
| data1 + data2     | $ O(n_1 + n_2) $ |
| C*data            | $ O(cn) $        |

#### 变异行为

> 列表类变异行为的渐近性能。下表中$n_i$表示对应列表的长度，*表示摊销操作下的时间复杂度。

| 操作                                   | 时间复杂度         |
| -------------------------------------- | ------------------ |
| data[j] = val                          | $ O(1) $           |
| data.append(value)                     | $ O(1)^* $         |
| data.insert(k, value)                  | $ O(n - k + 1)^* $ |
| data.pop()                             | $ O(1)^* $         |
| data.pop(k)<br />del data[k]           | $ O(n - k)^* $     |
| data1.remove(value)                    | $ O(n)^* $         |
| data.extend(data2)<br />data1 += data2 | $ O(n_2)^* $       |
| data.reverse()                         | $ O(n) $           |
| data.sort()                            | $ O(n\log n) $     |

#### 拓展列表

> Python提供了一种extend方法，该方法在作用上等同于遍历append，但相比于重复使用append，extend效率更高，其次，新建列表过程中，采用列表推导式与乘法初始化效率都比append要高。



## 栈、队列与双端队列

### 栈

> **栈是一系列对象组成的一个集合**，这些对象的插入与删除操纵遵循**后进先出(LIFO)**的原则。用户可以在任何时刻向栈中插入一个对象，但只能取得或者删除最后一个插入的对象（即“栈顶”）。

栈是最简单的数据结构，但它也是最重要的数据结构。从形式上而言，栈是一种支持以下两种操作的抽象数据类型(ADT)，用S表示这个ADT实例：

> - S.push(e):	将一个元素e添加到S的栈顶。
> - S.pop(e):     从栈S中移除并且返回栈顶的元素；如果此时栈为空的，该操作将报错。

此外，我们还定义了以下访问方法：

> - S.top():       在不移除栈顶元素的前提下，返回一个栈S的栈顶元素；如果此时栈为空的，该操作将报错。
> - S.is_empty(): 如果栈中不包括任何元素，则返回布尔值。
> - len(S)         返回栈S的元素数量。

我们可以简单通过list来实现一个栈，但是列表类不能代表正式的栈Stack类，特别是其append和push方法。相反，此处展示任何使用一个列表去实现栈元素的内部存储，并且提供一个符合堆栈的公共接口。

### 队列

> 队列是另一种基本的数据结构，它与栈互为“表亲”关系。队列是由一系列的对象组成的集合，这些对象的插入和删除遵循**先进先出(FIFO)**原则。

通常来说，队列的抽象数据类型定义了一个包含一系列对象的集合，其中元素的访问和删除被限制在队列的第一个元素，而元素的插入被限制在序列的尾部。对于序列Qeryan，队列的抽象数据类型支持以下两种基本方法：

> - Q.enqueue(e):	向队列Q的队尾添加一个元素；
>
> - Q.dequeue(e):	从队列Q中删除并返回第一个元素，如果队列为空，则触发一个错误。

此外，还定义了以下几种方法：

> - Q.first():	在不移除的前提下返回队列的第一个元素，如果队列为空，则触发错误；
>
> - Q.is_empty():	如果队列Q不包含任何元素，则返回布尔值“True”；
>
> - len(Q):	返回队列Q中元素的数量。

基于数组的队列实现，此处不做详细介绍。

### 双端队列

> 双端队列是一种类队列的数据结构，它同样是由一系列的对象组成的集合，不同的是，它支持在队列的头部和尾部都进行删除和插入操作。

为了提供一个类似的抽象，可以定义双端队列抽象数据类型D，这个抽象数据类型支持以下四种基本方法：

> - D.add_first(e):	向双端队列D的前面添加一个元素；
> - D.add_last(e):	向双端队列D的后面添加一个元素；
>
> - D.delete_first(e):	从双端队列D中删除并返回第一个元素，如果双端队列为空，则触发一个错误。
> - D.delete_last(e):	从双端队列D中删除并返回最后一个元素，如果双端队列为空，则触发一个错误。

此外，还定义了以下几种方法：

> - D.first():	在不移除的前提下返回双端队列D的第一个元素，如果双端队列为空，则触发错误；
> - D.last():	在不移除的前提下返回双端队列D的最后一个元素，如果双端队列为空，则触发错误；
>
> - D.is_empty():	如果双端队列D不包含任何元素，则返回布尔值“True”；
>
> - len(D):	返回双端队列D中元素的数量。

基于环形数组的双端队列实现，此处不做详细介绍。



## 链表

> 基于数组的序列和链表都能够队其中的元素保持一定的顺序，但采用的方式截然不同。数组提供更加集中的表示法，一个大的内存块能够位许多元素提供存储和引用。相对地，一个链表依赖于更多的分布式表示方法，采用称作**节点**的轻量级对象，分配给每一个元素。**每一个节点维护一个指向它的元素的引用，并含一个或多个指向相邻节点的引用**，这样做的目的时为了集中地表示序列的线性顺序。

### 单向链表

> 单向链表是最简单的实现形式就是由多个节点的集合共同构成一个线性序列。**每个节点存储一个对象的引用，这个引用指向序列的下一个元素。**

![image-20210821130209511](E:/工具/Typora/Temp/image-20210821130209511.png)

<center>单向链表存储格式图

链表的第一个节点和最后一个节点分别称为头节点和尾节点。通过next我们可以从头节点遍历到尾节点，当引用指向空的时候，我们可以确定该节点为尾节点，这个过程称为遍历链表，又因为引用可以视为指向下一个节点的链接或**指针**，遍历列表的过程又称为链接跳跃或**指针跳跃**。

单向链表一个重要的属性是没有预先确定的大小，它的占用空间取决于当前元素的个数。单向链表的任意位置都可以通过改变引用来实现元素的修改，为此，可以通过单向链表实现栈及队列。

### 循环链表

> 在链表中，我们可以使得链表的尾节点的“next”指针指向链表的头部，由此来获得一个更切实际的循环链表的概念，我们呈这种结构为循环链表。与标准链表相比，循环链表为循环数据集提供了一个更通用的模型，及标准链表的开始和结束没有特定的概念。

为了说明循环链表的使用，我们通过一个循环调度程序来认识循环链表，在这个调度程序中，以循环的方式迭代地遍历一个元素的集合，并通过执行给定的动作为集合中的每一个元素进行“服务”。同样的循环链表可以实现栈与队列。

### 双向链表

> 在单向链表中，每个节点为其后继节点维护一个引用，但这种链表的对称性很差，造成了一些不便之处，虽然单向链表可以方便地在头部及中间插入或删除一个节点，但是对于尾节点的删除却异常不便（为了访问上一个元素，需遍历整个链表），为此，为了提供更好的对称性，我们定义了一种双向链表。双向链表每个节点都为了指向其先驱节点以及后继节点的引用。

双向链表的边界都有追加有节点，在头部有头节点，尾部有尾节点，这些特定的节点称为哨兵或保安，这些节点中并不存储主序列的元素，仅存储单向引用。这样子，在使用少量的内存就能极大地简化操作的逻辑，最明显的，头尾哨兵从不改变。同时对于任意一个位置元素的插入或删除，仅需通过next和prev两个指针改变前后两个节点的引用即可完成。对于双向链表的实现与应用此处不赘述。

### 位置列表

#### 概述

在处理基于数组的序列（如Python列表）时，整数索引提供了一种很好的办法来描述一个元素的位置，或者描述一个即将发生插入和删除操作的位置。然而，数字索引并不适用于描述一个链表内部的位置，因为我们不能有效地访问一个只知道其索引的条目。找到一个链表中给定索引的元素，需要从链表的开始或者结束的位置逐个遍历从而计算出目标元素的位置。此外，索引会随着元素的删除或者不断改变。

**链表结构的好处之一是：只要给出列表相关的引用，它就可以实现在列表的任意位置执行插入和删除操作的时间复杂度都是 $ O(1) $ 。**因此，很容易开发一个ADT，它以一个节点引用实现描述位置的机制。 

#### 有序表

> 在常用的功能如收藏夹中，我们在链表中存储元素，以访问次数降序存储（使用插入排序法来找到每一个元素合理的位置），与常规的根据访问数量存储数据相比，以下提供了一种启发式动态调整列表。

**启发式算法**（又称经验法则），尝试利用**访问的局限性**（访问的局限性指，在许多实际的访问序列中，如果一个元素被访问，那么它很有可能在不久的将来再次被访问，这种情况称为访问的局限性。），就是在访问序列中采用Move-to-Front启发式。我们每访问一个元素，便把元素移动到元素的最前面，以便再次访问，但是相应地，在正常情况下，其访问速度更慢，正常访问其时间复杂度始终为 $ O(n^2) $ 。

对此，为了平衡这一缺陷，提出了**启发式算法的平衡算法**：

当要求寻找收藏夹列表中前k个访问最多的元素时，如果不再保存列表中通过访问次数排序的元素，就要搜索所有元素。实现top(k)方法的步骤如下：

> 1）将所有收藏夹列表中前k个访问最多的元素复制到另一个列表，并将其命名为temp；
>
> 2）扫描temp列表k次，每次扫描时，找出访问量最大的元素记录，并从temp中删除该元素同时返回结果报告。

实现top方法的时间复杂度为 $ O(kn) $ ，因此，当k为一常数时，top方法的运行时间复杂度为 $ O(n) $ 。 

### 总结

作为结尾，此处分析基于数组及基于链表两种不同形式的数据结构的优点。

|                            | 优点                                                         |
| -------------------------- | :----------------------------------------------------------- |
| 基于数组的序列             | ▪ **数组提供时间复杂度为 $O(1)$ 的基于整数索引的访问一个元素的方法；**对于任意k值的元素，数组访问都只需要 $ O(n) $ 的时间复杂度，而链表结构则需要$ O(k) $ 的时间复杂度，对于反向遍历双向链表，则需要$ O( n - k) $ 的时间复杂度。<br />▪ **通常，具有等效边界的操作使用基于数组的结构运行一个常数因子比基于链表的结构运行更有效率。<br />**▪ **相较于链式结构，基于数组表示使用的存储比较少。**一个数组大小为n的序列，基于数组的需要n个引用，而基于链表则至少需要2n个引用。 |
| <br />基于链表的序列<br /> | ▪ **基于链表的结构为它们的操作环境提供最坏情况的时间界限；**当许多单个操作时一个大型计算的一部分时摊销边界与最坏情况的边界一样精确，然而，如果数据结构操作作用域一个实时系统，旨在提供更迅速地反应，则单（摊销）操作导致的长时间延迟可能有不利影响。<br />▪ **基于链表的结构在任意位置进行时间复杂度为 $O(1)$ 的删除和插入操作。**<br />能够实现常数难度的时间复杂度的插入和删除操作，并通过Position有效地描述操作的位置，这可能是链表最显著的优势。 |



## 树

> 树是一种将元素分层次存储的抽象数据类型。除了最顶部的元素，每个元素在书中都有有个双亲节点和零个或多个孩子节点。通常，我们通过讲元素放置在有个椭圆形或者圆形中并且通过直线讲双亲结点与孩子节点相连来图示一棵树。我们通常称最顶部元素为树根。

### 正式定义

> 通常我们将树T定义为存储一系列元素的有限节点集合，这些节点具有parent-children关系并且满足如下属性：
>
> + 如果树T不为空，则它一定具有一个称为根节点的特殊节点，并且该节点没有父节点。
> + 每个非根节点v都具有唯一的父节点w，每个具有父节点w的节点都是节点w的一个孩子。

因此根据定义，一棵树可能为空，也可以通过递归来进行定义。

### 树的属性

#### 节点关系

> 同一个父节点的其他子节点之间是兄弟节点。一个没有孩子节点的节点v被称为外部节点。一个拥有多个孩子节点的节点v被称为内部节点。外部节点也叫叶子节点。
> 典型的，文件系统就是一个基于树的数据类型，树根即为根目录。

#### 边及路径

> 树T的边是指一对节点(u, v)，路径是指一系列的节点，这些节点中任意两个连续节点之间都是一条边。

#### 有序树

> 如果树中的每一个节点都有特定的顺序，则称该树为有序树，通常我们按照从上往下，从左往右的顺序进行排序。

### 抽象数据类型

我们用位置作为节点的抽象结构来定义树的抽象数据结构。一个元素存储在一个位置，并且位置信息满足树中的父节点与孩子节点的关系。一颗树的位置对象支持如下方法：

+ p.element():    返回存储在位置p中的元素

树的抽象数据类型支持如下访问方法。允许使用者访问一棵树的不同位置：

> + T.root(p):	返回树T的根节点位置。如果树为空，返回None。	
>
> + T.is_root(p):	如果位置p是树T的根，则返回True。
>
> + T.parent(p):	返回位置为p的父节点的位置。如果p的位置为树的根节点，则返回None。
>
> + T.num_children(p):	返回位置为p的孩子节点的编号。
>
> + T.children(p):	产生位置为p的孩子节点的一个迭代。
>
> + T.is_leaf(p):	如果位置p没有然后孩子，则返回True。
>
> + len(T):			返回树T所包含的元素的数量。
>
> + T.is_empty():	如果树T包含任何节点，返回True。
>
> + T.position():	生成树T存储的所有**位置**的迭代。
>
> + iter(T):			生成树T存储的所有**元素**的迭代。

### 深度与高度

假设p是树T中的一个节点，那么p的深度就是节点p的祖先个数，不包括p本身，同样，也可以通过递归来定义，此处不再展开介绍。

树T中节点p的高度定义为它的孩子节点中的最大高度加1，如果p是一个叶子节点，则它的高度是0。一颗非空树T的高度是树根节点的高度。（递归高度）

### 二叉树

二叉树是具有以下属性的有序树：

> 1）每个节点最多有两个孩子节点；
>
> 2）每个孩子节点内命名为左孩子或者右孩子；
>
> 3）对于每个节点的孩子节点，在顺序上，左孩子先于右孩子。

若自述的跟为内部节点v的左孩子或者右孩子，则该子树相应地被称为节点v的左子树或者右子树。除了最后一个叶节点的父节点外，若每个节点都有两个或者两个节点，则这样的二叉树被称为完全二叉树或者满二叉树，相应地，若二叉树不完全则称为不完全二叉树，我们可以通过递归来定义一个二叉树。

#### 抽象数据类型

作为抽象数据类型，二叉树是树的一种特殊化，其支持额外的三种访问方法：

> + T.left(p): 	返回p左孩子的位置，若没有左孩子，则返回None；
> + T.right(p):	返回p右孩子的位置，若没有右孩子，则返回None；	
> + T.sibling(p): 返回p兄弟节点的位置，若没有兄弟节点，则返回None。

#### 实现

对于树的内部表示，右几种不同的选择，此处介绍最普遍的表示方法，我们以二叉树为例介绍，因为二叉树的结构最具局限性。

##### 链式存储结构

> 实现二叉树T的一个自然方法是使用链式存储结构，一个节点包括多个引用，指向存储在位置p的元素的引用，指向p的孩子节点和双亲节点的引用。具体的Python实现，此处不介绍，可自行根据定义编写设计。
>
> 而对于一般树的链式存储结构，一个节点所拥有的孩子节点之间没有优先级限制。使用链式存储结构实现一般树T的一个很自然的方法是：使每个节点都配置一个容器，该容器存储指向每个孩子的引用。

##### 基于数组表示

二叉树T的一种可供选择的表示法是对T的位置进行编号。对于T的每个位置p，设 $ f(p) $ 为整数且定义如下：

> + 若p是T的根节点，则 $ f(p) = 0 $ ;
> + 若p是位置q的左孩子，则 $ f(p) = 2 f(q) + 1 $ ;
> + 若p是位置q的右孩子，则 $ f(p) = 2 f(q) + 2 $ ;

编号函数 $ f $ 被称为二叉树T的层编号，因为它将T的每一层的位置从左往右按递增顺序编号。注意，层编号是基于树内的潜在位置，而不是所给树的实际位置，因此，编号不一定是连续，对于没有节点的位置，其值为空，但是仍然占有位置。在Python中，我们可以直接使用列表来实现这一点。

基于数组表示的空间使用情况极大地依赖与树的形状，同时，基于数组表示的树，不能很好地支持树的一些更新方法。

### 遍历算法

> 树T的遍历是访问或者“拜访”T的所有位置的一种系统化方法。

#### 先序遍历

> 在树T的先序遍历中，**首先访问T的根**，然后递归地访问子树的根。如果这颗树是有序的，则根据孩子的顺序遍历子树。

#### 后序遍历

> 在某种程度上，这种算法可以看作相反的先序遍历，因为它**优先遍历子树的根**，即首先从孩子的根开始，然后访问根（因此叫后序）。

两种算法遍历的运行时间都是 $ O(n) $ ，都是最佳的，因为遍历必须经过树的n个位置。

#### 广度优先遍历

> 在访问树的位置时先序遍历和后序遍历是常见的方法，另一种常见的是遍历树的方法是在访问深度$d+1$ 的位置之前先访问深度 $ d $ 的位置，这种算法称为**广度优先遍历**。

#### 中序遍历

> 在中序遍历中，我们通过递归遍历左右子树去访问一个位置。专门应用于二叉树，可以看作“从左往右”非正式地访问T的节点。事实上，对于每个位置p，p将在其左子树之后及其右子树之前被中序遍历访问。

中序遍历算法一个重要应用便是把有序序列的元素存储在二叉树中，所定义的这种结构称为而茶搜索树。

---

### 树的应用

#### 目录表

> 我们使用树来表示文档的层次结构，树的先序遍历可以很自然地被用于产生一个文档的目录表。同时为了生成缩进，我们重新设置了一个自顶向下的递归，其中将当前的深度作为额外的参数。同时，通过递归共享相同的列表实例来实现层级的区分。

```python
def preorder_label(T, p, d, path):
    """Print labeled representation of subtree of T rooted at p at depth d."""
    label = ".".join(str(j+1) for j in path)
    print(2*d*" " + label, p.element())
    path.append(0)
    for c in T.children(p):
        preorder_label(T, c, d+1, path)
        path[-1] += 1
    path.pop()
```

#### 树的括号表示

> 如果之给定元素的先序序列，那么不可能重建一般的树。要更好地定义树的结构，一些上下文的附加是必要的，用缩进或者编号这种标签是非常人性化的表现，但是有些更简明的树的字符串是对计算机友好的。

```python
def parenthexize(T, p):
    """Print parenthesized representation of subtree of T at p."""
    print(p.element(), end="")
    if not T.is_leaf(p):
        first_time = True
       	for c in T.children(p):
            sep = "(" if first_time else ","
            print(sep, end="")
            firest_time = False
            parenthesize(T, c)
        print(")", end="")
```

#### 计算磁盘空间

> 我们用树作为文件系统结构的模型，用内部节点代表目录，用叶子代表文件。磁盘空间的递归计算是后序遍历的一个应用。

```python
def disk_space(T, p):
    """Return total disk space for subtree of T rooted at p."""
    subtotal = p.element().space()
    for c in T.children(p):
        subtotal += disk_space(T, c)
    return subtotal
```



## 优先级队列

> 优先级队列是一个包含优先级元素的集合，这个集合允许插入任何的元素，并且允许删除拥有**最高优先级(数字越小，优先级越大)**的元素。当一个元素被插入优先级队列中时，用户可以通过提供一个关联**键**来为该元素赋予一定的优先级。优先级最小的元素将是下一个从队列中被删除的元素。

### 抽象数据类型

我们形式化地将一个元素和它的优先级用一个key-value对进行建模。我们在优先级队列P上定义优先级队列ADT，以支持以下的方法：

> + P.add(k, v):	  向优先级队列P中插入一个拥有简直k和值v的元组。
> + P.min():	       返回一个优先级队列P中拥有最小键值的元组(k, v)；
> + P.remove_min():	返回并删除一个优先级队列P中拥有最小键值的元组(k, v)；如果队列为空，则会发生错误。
> + P.is_empty():	如果优先级队列不包含任何元组，将会返回布尔值"True"。
> + len(P):	          返回优先级队列元组的数量。

至于其实现，我们可以借助位置列表，其中，我们可以既可以采用排序的位置列表，也可以用不排序的位置列表，两种不同的方法有其各自的特点，排序后的位置列表支持的优先级队列，可以快速进行查询和删除，但是插入操作就比较慢，而未排序的位置列表则恰恰相反。

### 排序

#### 选择排序

> 选择排序，第一阶段将集合的元素顺序输入优先级队列P，随后第二阶段重复调用优先级队列P的自动排序功能输出返回原集合。

|       | 集合C                                                    | 优先级队列P                                              |
| ----- | -------------------------------------------------------- | -------------------------------------------------------- |
| 输入  | (1, 6, 4, 2)                                             | ()                                                       |
| 阶段1 | (6, 4, 2)<br />(4, 2)<br />(2)<br />()                   | (1)<br />(1, 6)<br />(1, 6, 4)<br />(1, 6, 4, 2)         |
| 阶段2 | ()<br />(1)<br />(1, 2)<br />(1, 2, 4)<br />(1, 2, 4, 6) | (1, 2, 4, 6)<br />(2, 4, 6)<br />(4, 6)<br />(6)<br />() |

如果用一个未排序的列表实现P，那么由于每增加一个元素都能在 $ O(1) $ 的时间复杂度内完成，所以第一阶段所花费的时间为 $ O(n) $ ，在第二阶段则与元素个数成正比，第二阶段的时间复杂度将变成算法的瓶颈，最坏情况下达到 $ O(n^2)$ 。

#### 插入排序

> 插入排序，正如之前所提到的，在第一阶段，顺序将集合C的元素插入到优先级队列P中，并且每次插入元素都对优先级队列P中的元素自右向左检查，是否大于其左侧的元素，若否，则交换两个元素的位置，直至完成这个集合的插入与排序，而在第二阶段，只需顺序输出至集合C即可。

|       | 集合C                                                    | 优先级队列P                                              |
| ----- | -------------------------------------------------------- | -------------------------------------------------------- |
| 输入  | (1, 6, 4, 2)                                             | ()                                                       |
| 阶段1 | (6, 4, 2)<br />(4, 2)<br />(2)<br />()                   | (1)<br />(1, 6)<br />(1, 4, 6)<br />(1, 2, 4, 6)         |
| 阶段2 | ()<br />(1)<br />(1, 2)<br />(1, 2, 4)<br />(1, 2, 4, 6) | (1, 2, 4, 6)<br />(2, 4, 6)<br />(4, 6)<br />(6)<br />() |

如果用一个未排序的列表实现P，在第一阶段插入排序的时间复杂度与元素个数成正比，而第二阶段所花费的时间为 $ O(n) $ ，第一阶段的时间复杂度将变成算法的瓶颈，最坏情况下达到 $ O(n^2)$ 。

#### 堆排序

堆实现的优先级队列优点在于：优先级队列ADT中的所有方法都以对数时间或者更短时间运行。因此，这种实现非常适合那些所有优先级队列都追求快速的运行时间的应用。

堆排序的第一阶段，通过使每个位置的键值都不小于其孩子节点的键值重新定义堆的操作，事情称为面向最大值的堆。此后，我们从一个空堆开始，从左校友移动堆和序列的边界，一次一步。

第二阶段下，我们从一个空的序列开始，并从右向左移动堆与序列的边界，一次一步，并且每次都将最大值元素从堆中移除并将其存储到索引为 $ n-1 $ 的位置上。

在这种情况下，整个算法的时间复杂度为 $ n \log n $ ，以达到性能上的最优。

### 适应性优先级序列

在一般的生活情境中，优先级序列便已经足够使用，但是在某些特殊情景下，则需要附加一些方法，如机场乘客的提前离开与会员插队。

为此，我们需要定义两个新的方法remove与update，分别用于删除特定位置的元组而不通过最高优先级删除与更新元组的优先级。

> + P.update(loc, k , v): 	用定位器loc代替键值作为元组的标识。
> + P.remove(loc):			从优先级队列中删除以loc为表示的特定元组，并返回键值对。

而这两个方法都涉及到对目标元组的定位，为了定位该元组的位置，我们新增一个**定位器对象**，通过在元组中增加一个额外的字段，该字段指定了基于数组表示的堆中元素的当前索引。至于其python实现，日后自行研究。

### 堆

> **堆是一颗二叉树**T，该树在它的位置上存储了集合中的元组并且满足两个附加的属性：关系属性以存储键的形式在T中定义；结构属性以树T自身形状的方式定义。

+ **Heap-Order属性**：在堆T中，对于除了根的每个位置p，存储在p中的键值大于或者等于存储在p的父节点的键值。在堆T中，从根到叶子节点的路径上的键值事以非递减顺序排序的（仅从上到下，从左到右的大小顺序则不限制）。
+ **完全二叉树属性**：一个高度为h的堆T是一颗完全二叉树，那么T的0，1，2...，h-1层上有可能达到节点数的最大值，并且剩下的节点在h级尽可能保存在最左的位置。

我们可以用堆来实现优先级队列，并且通常，我们通过**数组**来对完全二叉树进行表示，而关于其增加新元素及删除元素后的冒泡排序，此处不细做介绍，而关于堆的创建，通常采用**自下向上**创建一个堆，同时python内置模块**heapq模块**，提供堆基于堆的优先级队列的支持。



## 映射、哈希表和跳跃表

> dict类可以说是Python语言中最重要的数据结构。它表示一种称作**字典**的抽象，在其中每个唯一的关键字都被映射到对应的值上。由于所字典表示的键和值之间的关系，我们通常将其称为*关联数组(associate array)*或*映射(map)*。

### 抽象数据类型

我们引入映射ADT，并且定义其行为以使其与Python内建类dict一致。首先，我们列出了映射M基本的五类行为：

| Behavior | Description                                    | Method       |
| -------- | ---------------------------------------------- | ------------ |
| M[k]     | 如果存在，返回在映射M中与键k相对应的值         | \__getitem__ |
| M[k] = v | 将映射M中的键k与值k相关联                      | \__setitem__ |
| del M[k] | 从映射M中删除键为k的元组                       | \__deliter__ |
| len(M)   | 返回映射M中元组的数量                          | \__len__     |
| iter(m)  | 默认的对一个映射迭代生成器所包含的所有键的序列 | \__iter__    |

---

### 哈希表

> 哈希表是最常用的实现map的数据结构，而且Python还用它来实现dict类，哈希表通过哈希函数实现每一个键到值得映射，其中，我们将表概念化为桶数组。

#### 哈希函数

> 哈希函数h得目标就是把每个键k映射到 $[0, N - 1]$的区间内的整数，其中 $N$ 是哈希表的桶数组的容量。使用这种哈希函数的主要思想是使用哈希函数值 $ h(k) $ 作为哈希桶数组A内部的全部索引，而不是键k作索引，也就是说，我们在桶 $A[h(k)]$ 中存储元组 $(k, v)$ 。

如果有两个或者更多的键具有相同的哈希值，那么两个不同的元组将被映射到相同的桶A中。在这种情况下，我们称发生了一次冲突。虽然有很多办法可以解决冲突，但是最好还是避免这种情况的发生。

评价哈希函数的常见方法由两部分组成：哈希码与压缩函数，将哈希函数分成这样两个组件的优势是哈希码部分独立与具体的哈希表大小，这样就可以为每一个对象开发一个通用的哈希码，并且可以用于任何大小的哈希码，只有压缩函数与表的大小有关。

#### 哈希码

> 哈希函数执行的第一步是取出映射中的任意一个键k并且计算得到一个整数作为键k的哈希码；这个整数不需要再范围内，甚至可以是负数，同时我们希望分配到的哈希码尽可能避免冲突。下边是Python中哈希码的基中实现方式：

##### 将位作为整数处理

> **对于任何数据类型X使用尽可能多的位作为我们的整数哈希码，可以简单地把用于整数X的各个位作为它的哈希码。（对于整数，Python通常会这样做）**在Python中，哈希码是32位的，因此，对于64位的数据，需要将其高阶32位与低阶32位通过一定的方式进行合并，生成一个32位的哈希码。

##### 多项式哈希码

> 对于字符串和元组形式等可变长度的对象，上述方法不是一个好的选择，而一种可选的哈希码计算方法可以满足这样的需求：选择一个非零常数 $ a $ 且 $ a \ne 1$ ，并这样计算哈希码：
> $$
> x_0a^{n-1} + x_1a^{n-2} + \cdots\cdots +  x_{n-2}a + + x_{n-1}
> $$
> 从数学上，这仅仅是包含a并以表示对象x的元组 $(x_0, x_1, \cdots, x_{n-1})$ 中的元素为系数的一个多项式表示。因此这种哈希码称为多项式哈希码。

##### 循环移位哈希码

> 一个多项式哈希码的变种，是用一定数量的位循环位移得到的部分和来代替乘以a。比如一个32位数，将其左边5位移至最右边，得到新的结果作为哈希码。

在Python中计算哈希码的标准机制是一个内置签名 $ hash(x) $ 函数，这个函数将返回一个整型值作为对象x的哈希码。然而在Python中，只有不可变的数据类型是可哈希的。这个限制是为了确保在一个对象的生命周期期间，其哈希码保持不变。这是对于对象在哈希表中作为键的一个重要属性。

#### 压缩函数

> 通常，键k的哈希码不适合立即用于桶数组，因为整数哈希码有可能是负的或者超出桶数组容量的。因此，当我们决定对一个对象k的键使用整数哈希码时，还需要进行一次映射，这就是哈希函数的第二步，称为压缩函数。

##### 划分方法

一个简单的压缩函数就是直接进行划分，他将一个整数i映射到N：
$$
i \quad mod \quad N
$$
在这里N是一个固定的整数，此外，如果n不是素数，将会有很大的可能性造成冲突，因此，一个好的哈希函数，应当确保两个不同的键获取相同哈希的可能性为 $\frac{1}{N}$ 。

##### Mod方法

有一个更复杂的压缩函数可以帮忙一组整数键消除重复模式，即 Multiply-Add-and-Divide("或者MAD")方法，这个方法通过下面的函数来对i进行映射：
$$
[(ai + b)mod \quad p] mod \quad N
$$
其中，p是比N大的素数，a和b是从区间 [ 0, p-1] 任意选择的整数，选择这个压缩函数是为了消除在哈希集合中的重复模式。

---

### 冲突处理

由于有可能存在冲突，我们不能直接简单地将新的元组直接插入到桶中。

#### 分离链表

> 处理冲突的一个简单并有笑地方法是使每个桶 $ A[j] $ 存储其自身的二级容器，容器存储元组 $ (k, v) $。这种解决方法就叫分离链表。

![img](E:/工具/Typora/Temp/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NjM2NTAzMw==,size_16,color_FFFFFF,t_70.png)

#### 开放寻址

> 分离链表由很多很好的属性，但是它仍然需要一个链表作为辅助数据结构来保存存在冲突的元组，如果空间非常宝贵，则需要采用将每个元组直接存储到一个小的列表插槽中作为替代的方法。这种方法没有辅助结构，但它需要更加复杂度机制处理冲突，这种方法有几种变种，统称为开发寻址模式(又称为闭散列法)的解决方案，以下对几种常见的方法进行介绍。

##### 线性探测

使用开放寻址处理冲突的一个简单方法是线性探测。使用这种方法时，如果我们想要将一个元组$ ( k , v ) $ 插入桶 $ A [ j ] $ 处（这里 $ j = h ( k ) $ ），但是 $ A [ j ] $ 已经被占用，那么我们将尝试尝试插入 $ A [(j + 1) mod N]$ ，若 $  A[(j+1)\ mod\ N]$ 也已经被占用，则我们尝试使用 $ A[(j+2)\ mod\ N $ ，如此重复操作，直到找到一个可以接受新元组的空桶。

##### 二次探测

另一个开放寻址策略称为_二次探测_，它反复探测桶 $A[(h(k)+f(i))mod N], i=0,1,2,... $A，其中 $ f ( i ) = i^2 $，直到发现一个空桶。与线性探测一样，二次探测会使删除操作更复杂，但它确实可以避免在线性探测中发生的聚集模式。

##### 迭代探测

另一种避免聚集的开放寻址方法是迭代地探测桶 $ A[(h(k)+f(i)) mod N] $ ，这里 $ f ( i ) $ 是一个基于伪随机数产生器的函数，它提供一个基于原始哈希码位的可重复的但是随机的、连续的地址探测序列。python的字典类现在就是使用这种方法。

---

### 负载因子、重新哈希和效率

> 负载因子就是数据数量与桶个数的比值。负载因子接近1时，冲突发生的概率会急剧增加。使用分离链表时，我们一般令负载因子小于0.9。在开放寻址的方法中，负载因子过大还会导致聚集效应增强，使得找空位的时间大幅度增加。因此，我们应该让负载因子小于0.5。

#### 重新哈希

如果哈希表的插入操作引起了负载因子超过了指定的阈值，那么我们需要调整表的大小。将所有对象重新插入表总。哈希码不用改变，但需要重新设计一个新的压缩函数。新数组的大小至少为之前的一倍，通常设为两倍。

#### 哈希表的效率

重新哈希会导致\__setitem\_\_与\_\_getitem__摊销所增加的时间复杂度为O(1)。好的哈希函数，所有元组均匀分布。坏的哈希函数，将所有元组映射到一个桶中，这将导致，无论是使用开放寻址还是分离链表，操作的开销都是线性增长的。在实践中，哈希表使实现map函数最有效的方式之间。

网页参数中keys=values，往往是存储到map当中，因此，可以利用最坏情况下的哈希表从而对网页进行DOS攻击。

![2021051411212689](E:/工具/Typora/Temp/2021051411212689.png)

---

### 有序映射

> 传统的映射ADT允许用户查找与给定键的关联值，这种键的查找被称为精确查找。事实上，映射ADT基于哈希算法实现高性能，依赖于键的故意分散，一个有序映射的ADT拓展，其键应该是有序的，同时包括标准映射的所有行为，同时还包括以下行为：

| 操作                      | 作用                                                         |
| ------------------------- | ------------------------------------------------------------ |
| M.find\_min()             | 用最小键返回键值对，如果映射为空，则返回None                 |
| M.find\_max()             | 用最大键返回键值对，如果映射为空，则返回None                 |
| M.find\_It(k)             | 用严格小于k的最大键返回键值对，如果映射为空，则返回None      |
| M.find\_Ie(k)             | 用严格小于等于k的最大键返回键值对，如果映射为空，则返回None  |
| M.find\_gt(k)             | 用严格大于k的最小键返回键值对，如果映射为空，则返回None      |
| M.find\_ge(k)             | 用严格大于等于k的最小键返回键值对，如果映射为空，则返回None  |
| M.find\_range(start,stop) | 用start ≤ \\le ≤键 ≤ \\le ≤stop迭代遍历所有键值，一个为None，意味着一边为头 |
| iter(M)                   | 根据自然顺序从最小到最大，迭代遍历映射中的所有键             |
| reversed(M)               | 根据逆序迭代映射中的所有键                                   |

其两种常用应用为航班数据库和最大值库。

---

### 跳跃表

实现排序映射ADT的数据结构。关于搜索，排序数组允许以二分查找，所需时间为 $ O(\log n) $ ；然而，采用链表结构的话，则在最坏情况下，需要 $ O(n) $ 。关于更新，排序数组更新在最坏的情况下，时间复杂度为 $O(n)$，而链表则非常有效地支持更新操作。  

跳跃表，则是一个折衷的方法以有效地支持查找与更新。映射M的跳跃表S包含一列表序列。每个列表依照键的升序存储着M的一个元组子集。用两个标注为$-∞$和$+∞$的哨兵键追加元组。  

列表S还需满足的条件有：  

> 1. 列表 $S_0$ 包含映射M中的每一项（含$∞$）；  
>
> 2. 对于列表 $S_i$ 包含 一个列表 $S_{i-1}$ 随机生成的元组的子集 ；  
>
> 3. 列表 $S_h$ 仅包含无穷。

![2021051420480767](E:/工具/Typora/Temp/2021051420480767.png)

此外不再做详细介绍，可在需要时另行自学。



## 搜索算法

搜索算法，即是我们常说的查找算法，即在特定的序列中找到一个指定的元素，查找是常用的算法，除了最简单的顺序查找（以及有序数列使用的二分查找），还有基于二分查找的斐波那契查找(Fibonacci)等，此外，我们常用搜索树算法来实现该功能。

### 斐波那契搜索

斐波那契查找几乎与二分查找（有序数列）一模一样，只是在选取参照点的位置不同，二分法选取中点作为参照点，斐波那契查找则选取斐波那契点为参照点，也就是黄金分割点，实际上，常在斐波那契数列中找比黄金分割点略大的接近的点作为参照点，具体不再介绍，参照二分查找。

### 插值搜索

插值搜索与二分查找（有序数列）方法大体相同，不同的是对于参照点的选取，插值搜索找出一个大多数情况下的理论上最科学的参照点，这个点往往是 $ mid = left + (key - array[left]) * (right - left) // (array[right] - array[left]) $。其他步骤与二分查找一致，不再赘述。值得一提的是，对于元素分布比较广的序列，它的搜索效率并不理想。


### 二叉搜索树

> 树形数据结构的一个重要用途就是用作**搜索树**。此处，我们用搜索树结构来有效实现有序映射，映射M的三种基本方法上面已经有所介绍，不再赘述，而有序映射ADT包括很多其他附加功能，以保证迭代器按照一定的顺序输出键，并且支持额外的搜索。

#### 搜索

二叉搜索树的结构特性产生的最重要的结果时它的同名搜索算法（二叉搜索算法）。在这种算法下，对于每个节点的键k与预期键p进行判断，若是大于往左搜索，若是小于，往右搜索，若是相等则搜索成功终止，若最后得到空的子树，则没有搜索成功，也即无该键。

#### 搜索分析

二叉树搜索最坏运行时间的分析很容易。TreeSearch算法是递归的，并且每个递归调用恒定数量的基本操作。即，每个节点的搜索时间为 $ O(1)$ ，而每次搜索都会下降一层，总的高度为 $ h $， 因此总的运行时间为 $ O(h) $ 。

<div style="margin:0 calc(50% - 100px)">
<svg width=200px height=100px>
  <path d="M50 0 L100 100 L0 100 Z" style="fill:transparent;stroke:black"/>
  <path d="M100 0 L130 0" style="fill:transparent;stroke:black"/>  
  <path d="M105 100 L130 100" style="fill:transparent;stroke:black"/>
  <path d="M120 0 L120 100" style="fill:transparent;stroke:black"/>
  <path d="M112 8 L120 0 L128 8" style="fill:transparent;stroke:black"/>
  <path d="M112 92 L120 100 L128 92" style="fill:transparent;stroke:black"/>
  <path d="M50 0 L60 20 L50 38 L55 53 L50 60" style="fill:transparent;stroke:red"/>
  <circle cx=50 cy=70 r=2 />
  <circle cx=50 cy=76 r=2 />
  <circle cx=50 cy=82 r=2 />
	 <text x=0 y=30>树T</text>
  <text x=130 y=20>高度h</text>
</svg>
</div>


图示说明二叉搜索树的运行时间。其中将二叉树看作一个大三角形，那么从根节点开始的搜索路径就是该三角形内的锯齿形线。

#### 插入与删除

> 插入与删除二叉搜索树的项的算法都很常用，同时也很简单。

插入时会先进行检索，如果键存在，则该节点被重新赋值，如果键不存在，则在树T的下一层添加新的节点，代替搜索失败时得到的空子树。

从二叉搜索树T上删除一个节点比插入节点复杂，在删除时，首先检索该节点所在位置p，并按照以下两种情况处理：

1）如果p最多有一个孩子，则删除节点p并用子节点代替；

2）如果p有两个孩子，我们不能直接删除该节点，此时，先严格定义小于p处键的节点中拥有最大键的节点所在位置r，使用位置r的节点代替位置p节点，同时删除原来位置r的节点。

---

### 平衡树

> 在二叉搜索树中，我们注意到，如果有一系列随机的插入和删除操作，标准二叉树基本映射操作的运行时间是 $ O(\log n)$ 。但是，由于某些操作序列可能会生成高度与n 成比列的不平衡数，这种树的时间复杂度就是 $ O(n)$ 。

平衡二叉搜索树的主要操作是旋转。在旋转中，我们“旋转”大于其父亲节点的孩子节点。单个的旋转可以修改常熟数量的父子关系，同时改变树的形状并且维持树的性质。如果得当，可以避免非常不平衡的树结构。在一棵树内部，可以将一个或多个旋转合并来提供更广泛的平衡，这样的操作我们称为trinode重组。

---

### AVL树

---

### 伸展树

---

### 红黑树

较复杂，不再介绍。



## 排序与选择

> 讲到算法，最基础、简单的算法大概要数排序了。排序，简单地说就是将一个数组按照从小到大（或从大到小）的顺序重新排序。很简单的问题，却有很多经典的算法。

### 冒泡排序

> 冒泡排序(BubbleSort)是一种很原始的排序方法，就是通过不断底交换大数的位置达到目的的。因为这样不断出现大数类似于水泡不断出现，因此被形象称为冒泡算法。
>

#### 基本原理

冒泡排序的基本原理就是比较相邻两个数字的大小，将两数中比较大的那个数交换到靠后的位置，不断交换下去就可以将最大的那个数放到队列的尾部（每次可以得到第i大的数）。然后重头再交换（第i次交换，从1到n-i），直到将数列排成有序数列。

#### 代码实现

```python
def BubbleSort(array):
	size = len(array)
	if size <= 1:
		return array
	for i in range(1, size):
		for j in range(size - i):
			if array[j] > array[j+1]:
				array[j], array[j+1] = array[j+1], array[j]
	return array
```
该算法时间复杂度为 $ O(n^2) $， 比较笨。

---

### 选择排序

> 与冒泡排序相比，选择排序算法(SelectionSort)的原理更加简单粗暴，就是在数列中不断地找最大（小）值。
>

#### 原理

简单地说，选择排序只做一件事情，就是从数列中选择最大（最小）那个数，并从将这个数放到序列合适的位置，然后再在抛开该数的原数列找到最大（最小）值，重复上述步骤直至原序列为空。与冒泡序列有所不同，它不是比较相邻两个数的大小，而是所有的数。

#### 代码实现

```python
def SelectSort(array):
	size  = len(array)
	if size < 2 :
		return array
	for i in range(size-1):
		if array[i] != min(array[i:]):
			minindex = array.index(min(array[i:]))
			array[i], array[minindex] = array[minindex], array[i]
	return array
```
该算法本身理论上时间复杂度为 $ O(n^2) $，但是python中min比较特别，无序遍历全部，所以实际时间复杂度为 $ O(n) $，因此，在Python的排序算法中，选择算法是最快的 。但是上述代码实现比较差，无论是调用 min 函数（这其实是最佳算法，但是有点耍赖）还是通过index索引以及调序，其时间复杂度都是较大的，其空间复杂度暂且不知，其他算法可考虑创建一个子序列，通过append添加，其时间复杂度会减小不少，但是空间复杂度可能上升，具体不会分析，姑且如此，总体来说掌握原理即可。

### 插入排序

插入排序(InsertionSort)可能是最好理解的排序了。

#### 原理

插入排序首先将序列分成左右两个子序列，左字序列只包含一个元素，然后从右字序列中的元素逐个取出并插入左字序列的合适位置直至右子序列为空时，序列排序完成。

#### 代码实现

```python
def InsertionSort(array):
	size = len(array)
	if size < 2:
		return array
	for i in range(1, size):
		target = array[i]
		for j in range(i):
    	if target < array[i]:
    		array[i+1:j+1] = array[i:j]
    		array[i] = target
    		break
   return array
```
该算法时间复杂度为 $ O(n^2) $，和冒泡算法时间相当，一如既往，实现代码的风格不喜欢，仅供学习了解。

### 分治法

我们首先学习前两个算法——归并排序(MergeSort)和快速排序(QuickSort)，它们在称为分而治之的算法设计模式中使用了递归的方法，该设计模式包含以下三个步骤：

> 1）分解：如果输入值的规模小于确定的阈值，我们就通过直截了当的方法来解决这些问题并返回所获得的答案。否则，我们把输入值分解成两个或者更多的互斥子集。
>
> 2）解决子问题：递归地解决这些与子集相关的子问题。
>
> 3）合并：整理这些子问题的解，然后把它们合并成一个整体用以解决最开始的问题。

#### 归并排序

> **归并排序**，是创建在归并操作上的一种有效的排序算法，效率为$O(n \log n)$。最优时间复杂度为 $O(n)$ ，平均时间复杂度为 $O(n \log n)$ 。由下图我们可以了解到归并排序的过程。

##### 原理

归并排序首先将数列不断二等分直至不能再分为止，然后再两两合并，合并的同时比较每两个的大小排序，直至合成一个序列，此时序列也完成排序。

![img](E:/工具/Typora/Temp/webp.gif)

<center>归并排序法动态示意图（来自维基百科）

##### 代码实现

```python
def MergeSort(array):
	size = len(array)
	if size < 2:
  	return array
	middle = size // 2
	left, right = array[:middle], array[middle:]
	return MergeList(MergeSort(left), MergeSort(right))
	
def MergeList(left, right):
	temp = []
	while left and right :
		if left[0] > right[0]:
			temp.append(right.pop(0))
		else:
			temp.append(left.pop(0))
	while left:
		temp.append(left.pop(0))
	while right:
		temp.append(right.pop(0))
	return temp
这个代码更笨比，了解就好，一般递归的空间复杂度都比较大，总体上效率不高。同时，我们可以用一个二叉树T来形象化一个归并排序算法的执行过程，称这个二叉树为归并排序树。

#### 快速排序

> **快速排序**，是一种排序算法，最坏情况复杂度：$ Ο(n^2) $，最优时间复杂度：$ Ο(n \log n) $，平均时间复杂度：$ Ο(n \log n)$。快速排序的基本思想也是用了分治法的思想：找出一个元素X，在一趟排序中，使X左边的数都比X小，X右边的数都比X要大。然后再分别对X左边的数组和X右边的数组进行排序，直到数组不能分割为止。

##### 原理

> 1. 设置一个长度为n的数组A，定义两个变量 $ i = 0 $ ， $ j = n - 1 $ ;
> 2. 从数组中挑选出一个元素作为基准元素（往往是数组最后一位），复制给key；
> 3. 从 $j$ 开始从后向前搜索，$ j-- $ ，找到比key小的值，将 $ A[j] $与 $A[i] $互换；
> 4. 从i 开始向后搜索，$i++$，找到比key大的值，将$A[i]$与$A[j]$互换；
> 5. 递归的，重复2，3，4步，直到 $i == j$ ;

##### 代码实现
```python
def QuickSort(array):
	size = len(array)
	if size < 2:
		return array
	left, right = [], []
	for i in range(array[1:])
		if i <= array[0]:
			left.append(i)
		else:
			right.append(i)
	return QuickSort(left) + [array[0]] + QuickSort(right)
```
该算法效率较高，时间复杂度较理想，但其空间复杂度较大，少有的代码实现看着舒服的。

![img](E:/工具/Typora/Temp/webp.webp)

<center>快速排序法动态示意图（来自维基百科）

#### 随机快速排序

> 标准的快速排序在选取基准值时，每次都选取最右边的元素。但当**序列接近有序时或最值位于最右侧**时，会发现划分出来的两个子序列一个里面没有元素或者极少元素，而另一个则几乎包含了全部元素，在这种情况下，算法的效率就会大大降低。因此，为了避免这种情况，我们引入一个随机化量（也会占用时间与影响性能）来破坏这种有序状态来使得随机快速排序的时间复杂度接近期望 $O(n\log n)$ 。

在随机化的快排里面，选取 $a[left..right]$ 中的随机一个元素作为主元，然后再进行划分，就可以得到一个平衡的划分。

---

### 计数排序

计数排序是桶排序的一种特殊情况，可以把计数排序当成每个桶里只有一个元素的情况，它是**非比较排序算法**，它的优势在于在对一定范围内的整数排序时，它的复杂度为Ο(n+k)（其中k是整数的范围），快于任何比较排序算法。

#### 原理

对于一个输入数组中的一个元素 x ，如果这个数组中比 x 小的元素有 n 个，那么我们就可以直接把 x 放到(n+1)的位置上。这就是计数排序的基本思想，类似于哈希表中的直接定址法，在给定的一组序列中，先找出该序列中的最大值和最小值，从而确定需要开辟多大的辅助空间，每一个数在对应的辅助空间中都有唯一的下标。

基于这个思想，计数排序的一个主要问题就是**如何统计数组中元素的个数**。再加上输入数组中的元素都是 0 - k 区间的一个整数这个条件，那么就可以通过另外一个数组的地址表示输入元素的值，数组的值表示元素个数的方法来进行统计。

下面给出统计数组元素都是 0 - k 区间的整数的数组中各个元素个数的方法。

1. 找出序列中最大值和最小值，开辟大小为 $ Max - Min+1$ 的辅助空间；
2. 最小的数对应下标为 0 的位置，遇到一个数就给对应下标处的值 +1；
3. 遍历一遍辅助空间，就可以得到有序的一组序列。

![img](E:/工具/Typora/Temp/1122709-20180713181435644-2124541646-16300236263924.png)

#### 代码实现

```python
def CountingSort(array):
	size = len(array)
	if size < 2:
		return array
	temp = [None] * size
	for i in range(size):
		small, equal = 0, 0
   	for j in range(size):
   		if array[j] < array[i]:
   			small += 1
   		elif array[j] = array[i]
   			equal += 1
		temp[small, small + equal] = array[i] * equal
	return array
```
计数排序是一种以空间换时间的排序算法，并且只适用于待排序列中所有的数较为集中时，比如一组序列中的数据为 $0,1,2,3,\cdots,999$ ；就得开辟1000个辅助空间。 时间复杂度计数排序的时间度理论为 $O(n+k)$ ，其中 k 为序列中数的范围。 不过当 $O(k)$ > $O(n\log n)$ 的时候其效率反而不如基于比较的排序（基于比较的排序的时间复杂度在理论上的下限是 $O(n\log n)$）

---

### 桶排序

> 桶排序是计数排序的升级版。它利用了函数的映射关系（往往是利用哈希函数，起名来源正如哈希函数中的桶数组），高效与否的关键就在于这个映射函数的确定。桶排序 (Bucket sort)的工作的原理：假设输入数据服从均匀分布，将数据分到有限数量的桶里（往往是根据哈希函数键值分配N个桶，简单情况下如全两位数字数组，无需专门映射可按下图进行），每个桶再分别排序（有可能再使用别的排序算法或是以递归方式继续使用桶排序进行排）。其具体算法描述如下：

1. 设置一个定量的数组当作空桶

2. 遍历输入数据，并且把数据一个一个放到对应的桶里去
3. 对每个不是空的桶进行排序
4. 从不是空的桶里把排好序的数据拼接起来

![img](E:/工具/Typora/Temp/format,png.png)

<center>桶排序演示图片

桶排序最好情况下使用线性时间 $O(n)$ ，桶排序的时间复杂度，取决与对各个桶之间数据进行排序的时间复杂度，因为其它部分的时间复杂度都为 $O(n)$ 。很显然，桶划分的越小，各个桶之间的数据越少，排序所用的时间也会越少。但相应的空间消耗就会增大。因此，当键的值的范围N和序列大小n相比很小时，桶排序是高效的，但是当N与n的比值增大时，它的性能会降低。

桶排序的一个重要特性是：即使许多不同的元素有相同的键值，它也能得到正确的结果。

---

### 基数排序

> 基数排序是按照低位先排序，然后收集；再按照高位排序，然后再收集；依次类推，直到最高位。有时候有些属性是有优先级顺序的，先按低优先级排序，再按高优先级排序。最后的次序就是高优先级高的在前，高优先级相同的低优先级高的在前。其算法具体描述如下：

1. 取得数组中的最大数，并取得位数

2. arr为原始数组，从最低位开始取每个位组成radix数组

3. 对radix进行计数排序（利用计数排序适用于小范围数的特点）

![img](E:/工具/Typora/Temp/format,png.gif)

<center>基数排序演示图片

基数排序基于分别排序，分别收集，所以是稳定的。但基数排序的性能比桶排序要略差，每一次关键字的桶分配都需要 $O(n)$ 的时间复杂度，而且分配之后得到新的关键字序列又需要 $O(n)$ 的时间复杂度。假如待排数据可以分为d个关键字，则基数排序的时间复杂度将是 $O(d*2n)$ ，当然d要远远小于n，因此基本上还是线性级别的。

基数排序的空间复杂度为 $O(n+k)$ ，其中k为桶的数量。一般来说 $n>>k$ ，因此额外空间需要大概 $n $ 个左右。

---

### 排序算法总结

#### 排序稳定性

> 一个稳定的排序算法是指对于具有相同键的两个条目，排序前后其相对位置不发生变化。

#### 算法汇总

迄今为止，回顾一下现有的一些排序算法，比较各排序算法的复杂度、稳定性与性能。

![aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy9Gdzk1WmpjU0tlNllyV3FKSkpZZlJUMnVFVHA2eWRoU0x2dmZJVXd2MGtqaWNHcXpZbHB0U1Z0TWd2N0x1aWJ5ZFJGaWFCS3dnN01PdkFEaWNtSlh3SGFKY1EvNjQw](E:/工具/Typora/Temp/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy9Gdzk1WmpjU0tlNllyV3FKSkpZZlJUMnVFVHA2eWRoU0x2dmZJVXd2MGtqaWNHcXpZbHB0U1Z0TWd2N0x1aWJ5ZFJGaWFCS3dnN01PdkFEaWNtSlh3SGFKY1EvNjQw.jpg)

<center>现有的排序算法汇总

| 排序方法 | 时间复杂度（平均） | 时间复杂度（最坏） | 时间复杂度（最好） | 空间复杂度    | 稳定性 |
| -------- | ------------------ | ------------------ | ------------------ | ------------- | ------ |
| 插入排序 | $ O(n^2)$          | $ O(n^2)$          | $ O(n)$            | $ O(1)$       | 稳定   |
| 希尔排序 | $ O(n^{1.3})$      | $ O(n^2)$          | $ O(n)$            | $ O(1)$       | 不稳定 |
| 选择排序 | $ O(n^2)$          | $ O(n^2)$          | $ O(n^2)$          | $ O(1)$       | 不稳定 |
| 堆排序   | $ O(n\log n)$      | $ O(n\log n)$      | $ O(n\log n)$      | $ O(1)$       | 不稳定 |
| 冒泡排序 | $ O(n^2)$          | $ O(n^2)$          | $ O(n)$            | $ O(1)$       | 稳定   |
| 快速排序 | $ O(n\log n)$      | $ O(n^2)$          | $ O(n\log n)$      | $ O(n\log n)$ | 不稳定 |
| 归并排序 | $ O(n\log n)$      | $ O(n\log n)$      | $ O(n\log n)$      | $ O(n)$       | 稳定   |
|          |                    |                    |                    |               |        |
| 计数排序 | $ O(n + k)$        | $ O(n + k)$        | $ O(n + k)$        | $ O(n + k)$   | 稳定   |
| 桶排序   | $ O(n + k)$        | $ O(n^2)$          | $ O(n)$            | $ O(n + k)$   | 稳定   |
| 基数排序 | $ O(n * k)$        | $ O(n * k)$        | $ O(n * k)$        | $ O(n + k)$   | 稳定   |

<center>各种排序算法复杂度分析汇总

#### 插入排序分析

> 插入排序是一种进行**小序列排序**的优秀算法（比如数据规模小于50），其时间复杂度往往取决于序列中逆序的数量，因此对于接近有序的序列是很有效的，但是对于大数据规模，其时间复杂度则是很糟糕的。

#### 堆排序

> 当**输入的数据可以适应主存**时，堆排序就很容易地执行，并且在**小或者中型的序列**上是理所当然的选择。然而，堆排序在更大的序列上往往由于快速排序各归并排序。标准的堆排序由于元素的交换，并不能提供稳定排序。

#### 快速排序

> 由于分块中存在元素交换，快速排序同样不能提供稳定排序。几十年来，快速排序是一种通用的内存排序算法的默认选择。

#### 归并排序

> 对于**输入在计算机各级存储器层次结构之间被分层**的情况，归并算法是一个很优秀的算法。在这种语境下，归并排序在很长的合并流中处理数据的方法，最好地利用了在各级存储器的所有数据，因而减少了内存交换的次数，

#### 桶排序与基数排序

> 桶排序与基数排序对于一个**小的整型键、字符串或者来自离散范围的d元组键**对条目进行排序时是一个很好的选择，此时它的时间复杂度为 $ O(d( n + N))$ 。

---



### 选择

> 对于元素集合所要处理的各种顺序关系来说，排序并不是唯一有趣的问题。选择算法就是用来解决在一堆数里面选出第$k$大的数的问题。选择算法的设计方法有很多，比如将这堆数据先进行排序，然后取出对应的第k个元素就可以了，这种算法的平均运行时间为 $O(n\log n)$ ，说慢不慢，说快也不快。我们将在后面的内容里面介绍一种在线性时间 $O(n)$ 内就可以完成选择的算法。在这之前我们需要了解两个术语：
>

**顺序统计量**：第 $k$ 个顺序统计量就是在含有 $n$ 个元素的序列里面找到第 $k$ 大的元素。

**中位数**：中位数顾名思义就是找到序列里面排序在中间的那个数。

#### 剪枝搜索

我们可以在时间复杂度为 $O(n)$ 以内实现所有的k值解决选择问题，为了解决这个问题我们提出了一种剪枝搜索或者减治的设计模式。

基本思想：利用计算问题的特征，剪除不影响问题求解的输入数据，由剩下的输入数据构成一个与原问题形式相同，但规模更小的子问题，递归求解子问题得到原问题的解。

正确性：其正确性由剪枝策略的正确性保证。

算法复杂度分析：剪枝策略共剪除了 $λn$ 个输入数据，其中 $n$ 为问题的规模，则 $T(n)$ 满足： $T(n)=T((1−λ)n)+f(n)，对于 ∀ λ\in(0,1)$ 。


#### 快速排序算法

对于 Top K 问题，最简单的方案是把数组通过快排排序之后直接取对应的 k 值。它的时间复杂度是 $O(n \log n)$ ，空间复杂度是 $O(\log n) $ 。

如果我们仔细审查一下我们的问题的话，会发现Min-Heap 最小堆和快速排序算法都做了一些我们不需要的工作。

- 快速排序算法中，我们排序了数组中的所有值，通过排序后的数组，我们可以得到 Top 1， ...，Top K， Top K + 2 ... 的值，但实际上我们只需要 Top K 的值。
- Min-Heap 最小堆中，我们可以得到 Top 1，Top K - 1，Top K 的值，因为 k 一般都比数组长度小，所以我们能减少一些重复计算，但仍然重复计算了 Top 1，Top K - 1 等值。



## 文本处理

> 虽然多媒体信息很丰富，但文本处理依然是计算机的一个主要功能。计算机可用于编辑、存储和显示文件，并通过互联网传送文件。此外，数字系统用于归档广泛的文本信息，并且新数据正在以很快的速度增长。

### 模式匹配算法

> 在经典的模式匹配问题中，我们给出了长度为n的文本字符串T和长度为m的模式字符串P，并明确是否P是T的一个子串。如果是，则希望找到P在T中开始位置的最低索引。

#### 穷举

> 如要搜索或者优化某些功能，穷举算法设计模式是一种强大的技术。在一般情况下，运用这种技术时，我们通常会列举相关的所有可能情况，并挑出列举的所有情况的最优解。
>
> 穷举模式算法很简单。它由两个嵌套的循环组成：一个是在文本模式所有可能的开始索引进行外部循环索引；另一个是在模式的每个字符之间进行内部循环索引，并将它和文章中潜在对应的字符进行比较。另外，通过穷举搜索方法，穷举模式匹配算法的正确性立即就能得到保证。最坏的情况下，穷举模式匹配的运行时间很长，它要遍历文本字符串的每一个字符，因此，在最坏情况下，它的时间复杂度为 $ O(n m ) $ 。
>

#### Boyer-Moore算法

##### Description

起初，为了找出作为字串的模式P或者排除它存在的可能性，检查T中每个字符似乎时非常必要的。但并不总是如此。Boyer-Moore模式匹配算法优势可以避免对P和T中占很大比例的字符进行比较。

Boyer-Moore算法的主要思想时通过增加两个可能省时的启发式算法来提升穷举算法的运行时间：

> + 镜像启发式(looking-glass heuristic)：当测试P相对于T可能存在的位置时，可以从P的尾部开始比较，然后从后往前移动到P的头部。
> + 字幕跳跃启发式( character-jump heuristic)：在测试P在T中可能的位置时，有着相应模式字符P[k] 的文本字符 T[i] == c 的不匹配情况按下法进行处理。如果P中任何位置都不包括c，则将P完全移动到T[i]之后，否则，直到P中出现字符c并与T[i]一致才移动P。
> + 如果c在P中，last(c)是c在P中最后一次出现的索引；否则。默认定义last(c) = -1。
>

##### Python代码实现

```python
def find_boyer_moore(T, P):
	n, m = len(T), len(P)
	if m == 0 :
		return 0
	last = {}
	for k in range(m):
		last[ P[k]] = k
	i = m - 1
	k = m - 1
	while i < n:
		if T[i] == P[k]:
			if k == 0:
    	return i
			else:
				i -= 1	
				k -= 1
		else:
		 	j = last.get(T[i], -1)
		 	i += m - min(k, j+1)
		 	k = m - 1
	return -1
```


##### 性能

> 在最坏情况下，该算法的运行时间和穷举法一样，但实际上，在英文文本，不太可能有最坏情况，因为该算法能跳过英文文本中的绝大多数内容。
>

#### Knuth-Morris-Pratt算法

> Knuth-Morris-Pratt(KMP)算法，避免了信息的浪费，并且它能达到的运行时间为 $ O(n + m) $，这是渐近最有运行时间。即在最坏的情况下，任何模式匹配算法将会对文本的所有字符和模式进行一次检查。KMP算法的主要思想是预先计算模式部分之间的自重叠，从而当不匹配发生在一个位置时，我们在继续搜寻之前就能立刻知道移动模式的最大数目。
>

##### 失败函数

> 为了实现KMP算法，我们会预先计算失败函数f，该函数用于表达匹配失败时P对应的位移。具体地，失败函数 $ f(k) $ 定义为P的最长前缀的长度，它是 P[1: k+1]的后缀。直观地说，如果在字符P[k+1]中找到不匹配，函数  $ f(k) $ 会告诉我们多少紧接着的字符可以用来重启模式。
>
> 略略有些复杂，暂时跳过
>

---

### 动态规划

> 动态规划（Dynamic Programming），因此常用 DP 指代动态规划。本块内容我们主要讲解动态规划解题思路与动态规划问题类别。

#### LCS算法

LCS算法用于解决找出两个字符串的最长公共序列问题。

```python
def LCS(X, Y):
  """Return the max length of LCS."""
	n, m = len(X), len(Y)
	L = [[0] * (m + 1) for k in range(n+1)]
	for j in range(n):
		for k in range(m):
			if X[j] == Y[k]:
				L[j+1][k+1] = L[j][k] + 1
			else:
				L[j+1][k+1] = max(L[j][k+1], L[j+1][k])
	return L
```



---

### 文本压缩和贪心算法

> 文本压缩是根据一定的方法对大量数据进行编码处理以达到信息压缩存储的过程，被压缩的数据应该能够通过解码恢复到以前的状态，而不会发生信息丢失的现象。
>

#### Huffman编码

##### 1.原理简介

huffman压缩是数据结构课程中的常见内容, 是典型的贪心算法与二叉树的应用。

压缩前，以ascii文本为例，每个字符如a,b…都采用等长的8位acii码进行编码。huffman压缩的核心思想就是改为不等长编码，高频字符用短码表示，低频字符用长码表示，从而达到文本压缩的效果。为了能够顺利的解码，需要确保任意一个字符的编码不是另一个字符的前缀。否则如a编码01，b编码位010，那么在解码的过程中会出现岐义。因此问题转化为二叉树的最小带权外部路径长度问题。对于一个二叉树它的每个叶子对应一个字符具体编码，编码值由从根到叶子的路径决定，例如可以假定向左代表0，向右代表1。每个叶子有一个权重，可以是对应字符出现的次数或者频度。

算法过程是贪心的, 开始集合是所有的叶子节点权重, 每次取出两个权重最小的节点，合并为一个内部节点并将其放回集合中，直到集合中只有一个节点，根节点。即建好了这棵最优的二叉树，也即得到了所有字符的huffman编码。

##### 2.Huffman编码的优/缺点：

> huffman树的特性，任何一个编码绝不会是其他编码的前缀，这一点保证的编码译码的唯一性。它是一个简单而且实用的算法，但同时存在以下问题。

- 对于一些过短的文件进行 huffman编码的意义不大，因为我们存储huffman树的信息就需要1024bytes的空间；
- 对较大的文件进行编码，频繁的磁盘读写访问会降低数据编码的速度。
- 对文件的两次扫描。压缩时必须要知道每一个压缩字符在文本中出现的概率，所以要对文件中存储的字符进行两边扫描，第一遍计算每个字符在啊文版中出现的次数，创建出huffman树，在将 Huffman树的信息保存起来，以便解压缩创建同样额huffman树进行解压；第二遍是将文件中对应的字符转换为huffman编码存储到压缩文件中去。

#### 算术编码

> 算术编码是基于统计的、无损数据压缩效率最高的方法。它是从全序列出发，采用递推式的连续编码，他不是将单个信源符号映射成码子，而是将整段要压缩的整个数据序列映射到一段实数半封闭范围内的某一段区间，其长度等于该序列的概率。
>

**优点：**它避开用一个特定码字代替字符的思想，不需要传送huffman表，即_fileInfo,(自己实现的huffman文件压缩并没有传递该表，我是将信息全部放入压缩问件中，解压缩时从压缩文案中读取信息还原该表。）避免了huffman编码中比特位必须取整的问题。

**缺点：**很难在具有固定精度的计算机完成无限精度的算术操作；高度复杂的计算不利于实际应用；也需要两次扫描源数据流。

#### 基于字典的LZ系列编码

字典算法是将文本中出现频率较高的字符组合做成一个对应的字符字符列表，并用特殊的代码来表示字符。基于LZ序列的编码包括：LZ77算法、LZSS算法、LZ78算法、LZW等集中基本算法。LZ77和LZW算法实现起来很困难。

LZSS它是字典模式使用自适应模式，基本思路是搜索目前待压缩串是否在以前出现过，如果出现过则利用前次出现的的位置和长度来代替现在的待压缩串，输出该字符的出现位置及长度，否则输出新的字符串，从而起到压缩的目的。

> 优点：压缩算法的细节处理不同只能影响压缩率和压缩时间，对解压程序不会有影响
>
> 缺点：熟读问题，每次都需要向前索引到文件开头
>

#### 游程编码

通过统计带压缩数据中的重复字符、去出除文本中的冗余字符或字节中的冗余位从而达到减少数据文件中所占用的存储空间的目的。

当信源概率比较接近时，建议使用算术编码，因为huffman编码的结果趋于定长码，效率不高。

### 字典树

Trie树，即字典树，又称单词查找树或键树，是一种哈希树的变种，它是为了支持最快模式匹配的存储字符串的基于树的数据结构，典型应用是用于统计和排序大量的字符串（但不仅限于字符串），所以经常被搜索引擎系统用于文本词频统计。它的优点是最大限度地减少无谓的字符串比较。

Trie的核心思想是空间换时间。利用字符串的公共前缀来降低查询时间的开销以达到提高效率的目的。除了常用标准（前缀）树还有压缩树和后缀树。

<img src="E:/工具/Typora/Temp/Qu8z1f.png" alt="img" style="zoom: 33%;" />

#### 基本性质：

> 1. 根节点不包含字符，除根节点外每一个节点都只包含一个字符。
> 2. 从根节点到某一节点，路径上经过的字符连接起来，为该节点对应的字符串。
> 3. 每个节点的所有子节点包含的字符都不相同。

#### 字典树的基本操作

字典树的两种基本操作分别是**建树和查询**。其中建树操作就是把一个新单词插入到字典树里。查询操作就是查询给定单词是否在字典树上。

#### 插入操作

假如这个字典只包括26个英文字母（暂且都定为小写），那么这个树的深度会由具体单词不一样而定。但是它的广度范围是可以提前确定好的。对于每个节点，广度最大为26。（因为每个节点的下一个字母（即后缀点）只可能是26个字母。）那么我们可以用结构体开好这个“虚拟全树”（这个名字是笔者自己起的，请大家好好理解）。然后通过深度迭代向里面尝试加入单词。

我们开一个包含26个后缀指针的结构体。用变量now来表示指向当前节点编号的一个指针，用tot变量表示点的编号。end数组表示当前单词的“目标节点”即单词结尾的那个节点具体是哪个单词的词尾。

#### 查询操作

查询操作和刚刚的思路大同小异，因为我们已经有了一个“虚拟全树”，那么我们还是按深度向下迭代，对于需要查询的字符串的当前字符，如果这个对应的字符指针为空，就说明不含这个单词，直接跳出即可。当我们都迭代完成之后，直接返回end[now]即可。（注意，这里不能直接返回1或true，假如字典中只保存了一个字符串abcdef，而我们查询的是abc，它可以不被跳出地一直迭代到最后，但是它并不是字典中的单词。即，需要考虑字典中单词子串的情况）。



## 内存管理和B树

### 内存管理 

> 为了在实际计算机中实现任何数据结构，我们需要使用计算机的内存。计算机内存被组织成字序列，其中每个序列通常包括4、8或16字节（取决于计算机）。这些内存编号从 0 到 N - 1，其中 N 是计算机课获得的内存字节数量。与每个内存字节相关联的数字称为**内存地址**。为了运行程序和存储信息，必须对计算机的内存进行管理，以便确定什么样的数据被存储在哪个单元。
>

#### 内存分配

在Python中，所有对象都存储在内存池中，该内存池也被称为**内存堆**或**Python堆**（与数据结构中的堆不同）当执行Python程序时，Python解释器负责协调操作系统空间的使用和管理内存堆的使用。

内存堆存储空间被分成连续的块，类似于数组，块的大小可以是常量也可以是变量。系统必须快速实现该功能才能迅速为新对象分配内存。一种常用的方法是将连续空间的可用内存连接到链表上，称为**空闲链表**。只要它们的内存未被使用，这些空间就会被连接到链表。随着内存的释放和分配，空闲链表中的空间集就会发生变化，那些未使用的内存空间被已用的内存块分离成不相连的空间。未使用的内存分离成单独的空间，也被称为**碎片**。

可能产生的碎片有两种。当所分配的内存块的一部分未使用时，可能产生内部碎片。此外，当几个已分配内存的连续块之间有未使用的内存空间时，可能产生外部碎片。

为了最小化外部碎片，建议采用几种启发式算法来从堆中分配内存。**最佳适应算法**是搜索整个空闲列表以查找其大小最接近所请求内存的空间。**首次适应算法**是从空闲列表的首部开始搜索，直至搜索到一个足够大的空间， 但它每次搜索都从从前中断的地方开始，将空闲链表视为循环链表搜索。**最差适应算法**搜索空闲列表以找到最大的可用内存空间，如果该列表保存为优先级队列，灰比搜索整个空闲列表速度更快。

尽管最佳适应算法听起来很不错，但由于所选择空间的剩余部分偏小，故最容易产生外部碎片；首次适应算法快，但是往往在空闲列表前面产生很多外部碎片，这将降低之后的搜索速度；循环首次适应算法可以使得碎片更均匀地分布在整个内存堆从而降低了了搜索时间，但很难分配大的内存块。最差适应算法试图通过保留尽可能大的连续空间来降低这个问题。

### 垃圾回收

> 解释器负责检测“陈旧”对象的进程，释放用于这些对象的空间，并返回回收空间到空闲列表，这一过程称为垃圾回收。而在Python中，这一过程完全是由解释器完成。
>

要执行垃圾回收，就必须有对于不再使用对象的检测方法。Python的检测基于以下回收对象的保守原则。要访问程序的一个对象，它必须有该对象的直接或间接引用。我们将这种对象称为**活动对象**。在定义活动对象时，对象的直接引用是以标识符的形式存在于活跃的命名空间中，我们将所有这些具有直接引用的对象称为**根对象**。对象的间接引用是发生在一些其他活动对象的状态中的引用，如列表元素的引用。

Python通过以下策略来确定活动对象：

##### 引用计数

> 每个Python对象的状态都是一个整数，称为**引用计数**，即计算机系统中任何地方的对象有多少次引用。每一次引用赋给这个对象时，该对象的引用计数递增，每一次的引用被重新分配给其他对象时，原对象的引用计数就递减。

Python有专门用于该算法的程序，如果一个对象的计数减到0，那么该对象不可能是活动对象，因此能够立即释放该对象。

##### 周期检测

> 仅通过引用计数作为活动对象的判断标准是不充分的，有可能存在一组对象，相互引用但根对象是不可达的。

几乎每隔一段时间，特别是当内存堆中的可用空间变得越来越稀缺时，Python解释器就会使用垃圾回收的更高级形式来收回不可达的对象，尽管它们的引用计数非零。有不同的用于实现周期检测的算法，下面讨论一经典算法：
标记 - 清除算法。

在标记 - 清除算法中，我们设置一个“标记”位来标识每个对象是否是活动对象。当确定在某些时候需要垃圾回收，我们暂停所有其他活动，并清除当前在内存堆分配的所有对象的标志位，然后通过跟踪活跃的命名空间来标记所有根对象为活动对象。这个过程，往往是通过深度优先搜索算法完成的，又被称为“标记”阶段。一旦这个过程完成，再通过内存堆扫描并回收未被标记的对象正在使用的空间。这时，还有可以有选择地将内存堆中的分配空间合并成一个单独的块，从而暂时消除外部碎片。该扫描和回收过程称为“清除阶段”。当清除完成时，恢复运行暂停的程序。因此，标记 - 清除垃圾回收算法会按照活动对象的数量和其引用的数量加上内存堆的大小比例，及时回收未使用的空间，尽管如此，但在标记阶段，该算法面临一个重要问题，就是在内存不足回收内存时不能使用额外的空间。

### 存储器层次结构

> 为了容纳大数据集，计算机有不同类型的存储器结构层次，它们的大小和到CPU的距离各不相同，具体如下图所示。
>

![内存分级](E:/工具/Typora/Temp/untitled.png)

<center>内存分层情况

时间局部性：如果程序访问一个特定的内存位置，那么在不久之后的将来，它再次访问相同位置的可能性增加。

空间局限性：如果程序访问某个内存位置，它不久之后访问附近的其他位置的可能性会增加。

虚拟内存、高速缓存

点到为止，不再多说，反正也看不懂。

### 网页置换策略

浏览器通常会有m个“插槽”的缓冲存储器用于存储页面副本，若任意一个网页可以存储一个副本在任意一个位置，这样的高速缓存称为全相联高速缓冲存储器，当请求的页面位于高速缓存时，即可从缓冲调出副本快速访问，如果不在则需要重新访问并将其缓冲至缓存中，同时，如果缓存单元被耗尽时，就需要释放一些网页用于存储新的网页，其中一些较知名的页面置换策略：

> 先进先出策略(FIFO)：置换在主存中停留时间最长的页面，也就是在最远的过去传输到缓存的页面。
>
> 最近最久未使用策略(LRU)：置换在过去最远一次访问请求的页面。
>
> 随机策略：随机置换缓存中一个页面。

总的来说，LRU与FIFO都能更好地利用空间与时间局限性，但是与随机策略相比，它需要消耗相当多的内存。

